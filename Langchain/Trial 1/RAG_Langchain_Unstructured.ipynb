{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "873532fc-d6ad-4da6-b351-1c30543e836b",
      "metadata": {
        "id": "873532fc-d6ad-4da6-b351-1c30543e836b"
      },
      "source": [
        "# RAG Langchain with Unstructured PDF\n",
        "Name      : Willy Santoso<br>\n",
        "Subject   : RAG Langchain with Unstructured PDF\n",
        "<br><br>\n",
        "This code is can be run on Google Colab since this is a Jupyter Notebook file (upload this ipynb to Colab).<br>\n",
        "If using Google Colab, run this code below to verify Nvidia GPU Driver and install the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gQORtWUXS9-5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQORtWUXS9-5",
        "outputId": "c190879c-8688-4b90-92ba-3b5d0a9c77cd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CA7AYc5XInRK",
      "metadata": {
        "id": "CA7AYc5XInRK"
      },
      "source": [
        "#### Install PCI utils for Ollama GPU support, libs for Poppler PDF and Tesseract-ocr for image PDF reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NoLh1bBjTAbO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoLh1bBjTAbO",
        "outputId": "7aec9263-1a92-468a-c352-1307fd5aacf7"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -qq -y pciutils\n",
        "!sudo apt-get install -qq -y libxml2 libxslt1-dev libmagic-dev\n",
        "!sudo apt-get install -qq -y libnss3 libnss3-dev\n",
        "!sudo apt-get install -qq -y libcairo2-dev libjpeg-dev libgif-dev\n",
        "!sudo apt-get install -qq -y cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev\n",
        "!sudo apt-get install -qq -y tesseract-ocr\n",
        "!sudo apt-get install -qq -y libpoppler-dev poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NQSxJKYUTEJu",
      "metadata": {
        "id": "NQSxJKYUTEJu"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54930812-1982-4d88-9ff0-f8d4a93b2a2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54930812-1982-4d88-9ff0-f8d4a93b2a2f",
        "outputId": "e745138f-93f6-4651-82a5-a1fc4c05f359"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama serve > server.log 2>&1 &\n",
        "!ollama pull nomic-embed-text\n",
        "!ollama pull llama3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de043a8a-7587-4615-b051-ba821be34e53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de043a8a-7587-4615-b051-ba821be34e53",
        "outputId": "26800b42-923f-4b79-ace3-ea0f7458388a"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy==1.26.4\n",
        "!pip install -q protobuf==4.25.4\n",
        "!pip install -q chromadb==0.4.24\n",
        "!pip install -q onnx==1.16.1\n",
        "!pip install -q onnxruntime==1.17.1 onnxruntime-gpu==1.17.1\n",
        "!pip install -q rapidocr-onnxruntime\n",
        "!pip install -q datasets==2.18.0\n",
        "!pip install -q pytesseract\n",
        "!pip install -U -q nltk\n",
        "!pip install -U -q langchain langchain-core langchain-community\n",
        "!pip install -U -q langchain-chroma langchain-ollama langchain-huggingface langchainhub langserve langsmith\n",
        "!pip install -U -q langchain-unstructured unstructured-client unstructured \"unstructured[all-docs]\" python-magic pydantic lxml pypdf pymupdf\n",
        "!pip install -U -q ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XoJkYdYkRgkq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoJkYdYkRgkq",
        "outputId": "0e2dcabf-3d03-4fc1-f7bc-2eac757637e1"
      },
      "outputs": [],
      "source": [
        "!wget -O Attention-is-all-you-need.pdf \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jvarsY2lUess",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvarsY2lUess",
        "outputId": "4e423645-fb95-4d40-930f-ea58c099eb9d"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0daa64-7114-4666-a329-320eba96105a",
      "metadata": {
        "id": "dd0daa64-7114-4666-a329-320eba96105a"
      },
      "source": [
        "### Table of Contents\n",
        "1. Data Preprocessing\n",
        "2. Retrieval Strategy\n",
        "3. Model Selection\n",
        "4. Evaluation Dataset Creation\n",
        "5. Evaluation\n",
        "6. Recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b0ef91-dcf5-4b0a-bd38-7774709a69df",
      "metadata": {
        "id": "a8b0ef91-dcf5-4b0a-bd38-7774709a69df"
      },
      "source": [
        "### 1. Data Preprocessing\n",
        "Here we are preparing the required libraries and load the PDF with `UnstructuredPDFLoader`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef37cd52-9ed5-4a62-8de0-5af07b1f476c",
      "metadata": {
        "id": "ef37cd52-9ed5-4a62-8de0-5af07b1f476c"
      },
      "source": [
        "<b>Import the required libraries</b><br>\n",
        "- Langchain: main library/tools for RAG\n",
        "- Chroma: for storing vector databases that generated from Embeddings\n",
        "- RecursiveCharacterSplitter: use the recursive text splitter for splitting documents\n",
        "- UnstructurdPDFLoader: for loading unstructured documents like PDFs.\n",
        "- ChatOllama: use open-source Ollama for Chat LLMs.\n",
        "- Ollama Embeddings: use open-source Ollama for Embeddings.\n",
        "- ChatPromptTemplate and PromptTemplate: template for inserting the prompts.\n",
        "- StrOutputParser: string output parsers.\n",
        "- RunnablePassthrough: runnable for passing the question inputs from the user.\n",
        "- MultiQueryRetriever: the main Retrieval method that we want to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b1237d-57d4-4094-a9d8-0cb9d764e68f",
      "metadata": {
        "id": "c4b1237d-57d4-4094-a9d8-0cb9d764e68f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd741702-dcb4-4777-be44-a14283ceac18",
      "metadata": {
        "id": "bd741702-dcb4-4777-be44-a14283ceac18"
      },
      "source": [
        "<b>Load the document</b><br>\n",
        "Let's load the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa31770-a569-4048-8a95-fb35b730ba77",
      "metadata": {
        "id": "7aa31770-a569-4048-8a95-fb35b730ba77",
        "tags": []
      },
      "outputs": [],
      "source": [
        "loader = UnstructuredPDFLoader('./Attention-is-all-you-need.pdf')\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45537625-d24c-4637-9c53-7cd3fa719be5",
      "metadata": {
        "id": "45537625-d24c-4637-9c53-7cd3fa719be5"
      },
      "source": [
        "Display head of paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be4beb0-4997-49ee-bfbd-74a4039d6fe8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1be4beb0-4997-49ee-bfbd-74a4039d6fe8",
        "outputId": "67e8f23c-45db-48a4-c488-b7b3e87a5ee1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(data[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b95f8b1-1108-4c39-ad94-f6c7f2cfff67",
      "metadata": {
        "id": "4b95f8b1-1108-4c39-ad94-f6c7f2cfff67"
      },
      "source": [
        "<b>Splitting document</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37bde021-fef8-4a77-80fe-0e43287ff14a",
      "metadata": {
        "id": "37bde021-fef8-4a77-80fe-0e43287ff14a"
      },
      "source": [
        "For Splitting documents, there is two known method from Langchain:\n",
        "- `CharacterTextSplitter`: This is a simpler method that splits the text based on a specified character, such as spaces or newlines.\n",
        "- `RecursiveCharacterTextSplitter`: This method is more advanced and versatile. It attempts to split the text using a series of separators in a hierarchical or recursive manner. For example, it might first try to split the text at paragraph breaks (\\n\\n), and if the resulting chunks are too large, it then tries to split by single newlines (\\n), and if necessary, by spaces, and finally by individual characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df78aee-98ca-4636-b5d1-bf63e0543d75",
      "metadata": {
        "id": "4df78aee-98ca-4636-b5d1-bf63e0543d75",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Split and chunk\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd30db3b-7494-4244-b180-9c053b7a7d40",
      "metadata": {
        "id": "dd30db3b-7494-4244-b180-9c053b7a7d40"
      },
      "source": [
        "### 2. Retrieval Strategy\n",
        "For Retrieval Strategy: here we will be using Dense Retrieval (Embedding Vectors) and `MultiQueryRetriever`, it can send multiple queries at one time and one prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb84bb07-2288-49fc-af9b-64f349af606a",
      "metadata": {
        "id": "cb84bb07-2288-49fc-af9b-64f349af606a"
      },
      "source": [
        "<b>Adding Vector Databases to ChromaDB</b><br>\n",
        "We will using `OllamaEmbeddings` with model: `nomic-embed-text`<br>(open-source Embedding model that claimed surpasses OpenAI `text-embedding-ada-002` and `text-embedding-3-small`, <a href=\"https://ollama.com/library/nomic-embed-text\">[source]</a>)\n",
        "<br>\n",
        "Then we need to store the Embedding vectors into Vector Database from ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e5e371c-8de5-438e-9fbb-2f3f5c754b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e5e371c-8de5-438e-9fbb-2f3f5c754b69",
        "outputId": "609a1278-2308-4318-85ca-a62d9e948c97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Add to vector database\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
        "    collection_name=\"attention-paper-rag\",\n",
        "    persist_directory=\"./local-rag-attention\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "434a9d88-63a6-42f9-8b27-6afa00c25acc",
      "metadata": {
        "id": "434a9d88-63a6-42f9-8b27-6afa00c25acc"
      },
      "source": [
        "### 3. Model Selection\n",
        "Load the LLM Chat model, we will be using the latest Llama3.1: open-source LLM that improved from Llama3.<br>\n",
        "For detailed bechmarks and comparisons, here: https://blog.gopenai.com/llama-3-1-vs-llama-3-differences-d3d23e09607f\n",
        "\n",
        "Why use open-source LLMs? Well, there's several reasons:\n",
        "- Deployment can be local and anywhere\n",
        "- No need to pay or subscription services\n",
        "- Privacy matters, as open-source LLMs runs on local, privacy stays on local machine\n",
        "- Open-source models has worldwide contributions unlike closed-source is classified contributions\n",
        "- Open-source has massive support of community developers like HuggingFace, llama.cpp, Ollama, Langchain, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16d066a-7e06-4aa3-a554-545159d48848",
      "metadata": {
        "id": "b16d066a-7e06-4aa3-a554-545159d48848",
        "tags": []
      },
      "outputs": [],
      "source": [
        "llm = ChatOllama(model='llama3.1')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16da44cb-3125-466e-bef3-d3b2c48455b0",
      "metadata": {
        "id": "16da44cb-3125-466e-bef3-d3b2c48455b0"
      },
      "source": [
        "Define the Query Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c545fbd-e713-4163-87e8-d6d721152e49",
      "metadata": {
        "id": "8c545fbd-e713-4163-87e8-d6d721152e49",
        "tags": []
      },
      "outputs": [],
      "source": [
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from\n",
        "    a vector database. By generating multiple perspectives on the user question, your\n",
        "    goal is to help the user overcome some of the limitations of the distance-based\n",
        "    similarity search. Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7db72f-cb84-41a5-8c24-31d1c127ad04",
      "metadata": {
        "id": "ec7db72f-cb84-41a5-8c24-31d1c127ad04",
        "tags": []
      },
      "source": [
        "<b>Still about Retrieval Strategy</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c0dfca-443c-4fc0-8eaf-f079a1028cfb",
      "metadata": {
        "id": "f4c0dfca-443c-4fc0-8eaf-f079a1028cfb"
      },
      "source": [
        "We will be using `MultiQueryRetriever` for main strategy, as it can send multiple queries at one time and one prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef1c21e-e39f-4b31-a561-daf8dcedcc2a",
      "metadata": {
        "id": "2ef1c21e-e39f-4b31-a561-daf8dcedcc2a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    vector_db.as_retriever(),\n",
        "    llm,\n",
        "    prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "# RAG prompt\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad54b4b-5bc2-4776-8d1b-200df347dddb",
      "metadata": {
        "id": "1ad54b4b-5bc2-4776-8d1b-200df347dddb"
      },
      "source": [
        "Create the Retrieval Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2c0ec5-373a-4fcc-b3ad-7600e442f4b7",
      "metadata": {
        "id": "5e2c0ec5-373a-4fcc-b3ad-7600e442f4b7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea17868-c004-4afd-a7ea-087230ea86e2",
      "metadata": {
        "id": "9ea17868-c004-4afd-a7ea-087230ea86e2"
      },
      "source": [
        "### 4. Evaluation Dataset Creation\n",
        "Develop an evaluation dataset that reflects realistic queries researchers might have. Include both simple and complex queries (multi-hop, comparing two things, multiple questions in one prompt) that test the system's ability to retrieve and generate accurate information. Around 20 questions should be enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04f280a-ffb6-40ad-aa9c-65e33057198a",
      "metadata": {
        "id": "a04f280a-ffb6-40ad-aa9c-65e33057198a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"What is the main contribution of the Transformer model?\",\n",
        "    \"What does the Scaled Dot-Product Attention mechanism do?\",\n",
        "    \"How many layers are there in the encoder stack of the Transformer?\",\n",
        "    \"Explain the architecture of the Transformer model?\",\n",
        "    \"What tasks were used to evaluate the Transformer model?\",\n",
        "    \"What are the differences between the encoder and decoder stacks in the Transformer model?\",\n",
        "    \"What are the advantages of using self-attention in the Transformer model?\",\n",
        "    \"How does the Transformer model handle positional information in sequences?\",\n",
        "    \"Explain the regularization techniques used in training the Transformer\",\n",
        "    \"How do the training schedules differ between the base and big Transformer models?\",\n",
        "    \"Compare the training efficiency of the Transformer with RNN-based models\",\n",
        "    \"How does the positional encoding in the Transformer differ from other models?\",\n",
        "    \"How does the Transformer handle long-range dependencies compared to convolutional networks?\",\n",
        "    \"What is the purpose of the positional encoding in the Transformer?\",\n",
        "    \"What hardware was used to train the Transformer models?\",\n",
        "    \"How many attention heads are used in the Transformer?\",\n",
        "    \"What is the main challenge that self-attention addresses in sequence modeling?\",\n",
        "    \"What is the effect of using multi-head attention as shown in the paper?\",\n",
        "    \"Describe the key components of the Transformer's encoder and decoder stacks\",\n",
        "    \"What training data was used for the Transformer model?\",\n",
        "]\n",
        "ground_truths = [\n",
        "    \"The Transformer have ability to achieve state-of-the-art results without relying on recurrent neural networks (RNNs) or convolutional layers, relying only on attention mechanisms\",\n",
        "    \"The Scaled Dot-Product Attention maps a query and a set of key K and value V pairs to an output, computed as a weighted sum of the values.\",\n",
        "    \"The encoder stack consists of 6 identical layers.\",\n",
        "    \"The Transformer model consists of an encoder-decoder architecture, with each having a stack of 6 identical layers utilizing self-attention and fully connected feed-forward network.\",\n",
        "    \"The Transformer model was evaluated on two machine translation tasks: WMT 2014 English-to-German and WMT 2014 English-to-French.\",\n",
        "    \"The encoder consists of two sub-layers: self-attention and a feed-forward network. The decoder includes an additional third sub-layer for encoder-decoder attention.\",\n",
        "    \"Self-attention allows for parallelization, reduces path lengths, and enables the model to learn long-range dependencies effectively.\",\n",
        "    \"The Transformer uses positional encodings based on sine and cosine functions to inject positional information into the sequence.\",\n",
        "    \"Regularization techniques include residual dropout, label smoothing, and weight sharing across embedding layers.\",\n",
        "    \"The base model is trained for 12 hours, and the big model for 3.5 days, with corresponding increases in training steps.\",\n",
        "    \"The Transformer is significantly more parallelizable and trains faster than recurrent-based models, achieving better and faster performance with less computational resources.\",\n",
        "    \"The Transformer uses fixed sine and cosine functions, whereas other models might use learned positional embeddings.\",\n",
        "    \"The Transformer’s self-attention mechanism allows it to capture long-range dependencies with a constant number of sequential operations, unlike convolutional networks that are limited to a local window and require stacking multiple layers.\",\n",
        "    \"Positional encoding is used to inject information about the relative or absolute position of the tokens in the sequence.\",\n",
        "    \"The Transformer models were trained using 8 NVIDIA P100 GPUs.\",\n",
        "    \"The Transformer employs 8 parallel attention heads in its multi-head attention mechanism.\",\n",
        "    \"Self-attention addresses the challenge of modeling dependencies without requiring their distance in the input or output sequences.\",\n",
        "    \"Multi-head attention allows the model to focus on different positions within the sequence, providing a more nuanced understanding by attending to different subspaces.\",\n",
        "    \"The encoder consists of two sub-layers: self-attention and position-wise feed-forward networks, while the decoder adds encoder-decoder attention to these components.\",\n",
        "    \"The WMT 2014 English-to-German and English-to-French datasets were used for training the Transformer.\"\n",
        "]\n",
        "contexts = []\n",
        "answers = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1166c79-5a91-4e34-a9ff-c9d2e3a4b1cd",
      "metadata": {
        "id": "b1166c79-5a91-4e34-a9ff-c9d2e3a4b1cd"
      },
      "source": [
        "<b>Question no.1</b><br>\n",
        "Simple Queries with MultiQueryRetriever: \"What is the main contribution of the Transformer model?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97468c7b-62e9-4974-a6cd-b3246f65c575",
      "metadata": {
        "id": "97468c7b-62e9-4974-a6cd-b3246f65c575",
        "outputId": "4c1bc906-b09a-4b20-9dea-75fc29e81dfe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "content = []\n",
        "retrieval = retriever.invoke(\"What is the main contribution of the Transformer model?\")\n",
        "\n",
        "for item in retrieval:\n",
        "    content.append(item.page_content)\n",
        "contexts.append(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb4e6e2-4321-4923-8f66-a28156f3850c",
      "metadata": {
        "id": "2bb4e6e2-4321-4923-8f66-a28156f3850c",
        "outputId": "1988c37f-5a9b-4ee1-b2fd-bb29f789e649",
        "tags": []
      },
      "outputs": [],
      "source": [
        "answer = chain.invoke(\"What is the main contribution of the Transformer model?\")\n",
        "answers.append(answer)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35157cb1-af7a-44da-909c-b9665c9116b1",
      "metadata": {
        "id": "35157cb1-af7a-44da-909c-b9665c9116b1"
      },
      "source": [
        "### 5. Evaluation\n",
        "Here we will be using RAGAS evaluation metrics for evaluating RAG results.<br>\n",
        "There are several RAGAS metrics:<br>\n",
        "1. Faithfullness: Faithfulness measures how factually consistent the generated answer is with the provided context. It ensures that the claims made in the answer can be directly inferred from the given context. A faithful answer means that all statements in the answer are supported by the context.<br>(Example: If the context states that \"Einstein was born on March 14, 1879,\" an answer saying he was born on March 20, 1879, would have low faithfulness.)<br>\n",
        "2. Answer Relevancy: This metric assesses how relevant the generated answer is to the original question. It focuses on whether the answer addresses the query directly and avoids unnecessary or unrelated information. Relevance is often measured by generating variations of the question based on the answer and comparing them to the original question using cosine similarity.<br>(Example: For a question asking \"What is the capital of France?\", an answer stating \"Paris is the capital of France\" is highly relevant, while an answer discussing French cuisine would score lower.).<br>\n",
        "3. Context Precision: Context precision evaluates how accurately the retrieved context provides the necessary information to answer the question. It measures whether the relevant information is ranked higher among the retrieved documents, focusing on the signal-to-noise ratio in the retrieval process.<br>(Example: If the context retrieved is directly related to the query without much irrelevant information, the precision is high.).<br>\n",
        "4. Context Recall: Context recall measures how much of the relevant information from the ground truth is included in the retrieved context. It compares the retrieved context with the expected answer to see if all necessary details are captured.<br>(Example: If the ground truth for a question includes multiple details, and the retrieved context covers most of these, the recall would be high.).<br>\n",
        "5. Context Entity Recall: This metric is similar to context recall but focuses specifically on entities mentioned in the context. It checks if the retrieved context includes all the entities that are crucial to answering the question.<br>(Example: If a query is about \"Albert Einstein,\" the context entity recall checks if the retrieved documents mention Einstein.).<br>\n",
        "6. Answer Similarity: Answer similarity assesses how close the generated answer is to the ground truth by comparing the semantic similarity between the two. This metric ensures that even if different words are used, the meaning conveyed is similar.<br>(Example: If the ground truth answer is \"Paris is the capital of France,\" and the generated answer is \"The capital of France is Paris,\" the similarity would be high.).<br>\n",
        "7. Answer Correctness: This metric evaluates the factual correctness of the generated answer by comparing it to the ground truth. It considers both the factuality and the semantic similarity of the answer to ensure it is correct.<br>(Example: If the ground truth states \"Paris is the capital of France,\" and the answer says \"London is the capital of France,\" the correctness would be low.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8027e0d7-101a-4823-ae13-e1f89c530c82",
      "metadata": {
        "id": "8027e0d7-101a-4823-ae13-e1f89c530c82",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    context_entity_recall,\n",
        "    answer_similarity,\n",
        "    answer_correctness,\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "d = {\n",
        "    \"question\": queries,\n",
        "    \"answer\": answers,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truth\": ground_truths\n",
        "}\n",
        "dataset = Dataset.from_dict(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6174ce12-1e2e-4450-acb8-3485e98574a6",
      "metadata": {
        "id": "6174ce12-1e2e-4450-acb8-3485e98574a6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "score = evaluate(dataset,\n",
        "                 metrics=[faithfulness, answer_relevancy, context_precision, context_recall, context_entity_recall, answer_similarity, answer_correctness],\n",
        "                 llm=ChatOllama(model='llama3.1'),\n",
        "                 embeddings=OllamaEmbeddings(model='nomic-embed-text'))\n",
        "score_df = score.to_pandas()\n",
        "score_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54891b06-8809-4c40-b15e-3384fdedcc92",
      "metadata": {
        "id": "54891b06-8809-4c40-b15e-3384fdedcc92"
      },
      "source": [
        "The evaluation scores can be exported to CSV format if decided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b3bd05-85b6-453d-bd32-0300acbc102a",
      "metadata": {
        "id": "04b3bd05-85b6-453d-bd32-0300acbc102a"
      },
      "outputs": [],
      "source": [
        "score_df.to_csv(\"EvaluationScores.csv\", encoding=\"utf-8\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39d5c51-d864-45db-afc6-3f5905effddd",
      "metadata": {
        "id": "a39d5c51-d864-45db-afc6-3f5905effddd"
      },
      "source": [
        "### 6. Recommendation\n",
        "From the Evaluation Dataset Creation result, it seems the strategy needs to be improved. As the answers from LLMs there is correct information but also there is some hallucination and over-typing the messages. Also, the Document PDF Loader needs to be improved too, as another approach (Semi-Structurd PDF Loader <a href=\"https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb\">from here</a>) will be better preprocessing the documents (but i'm facing issues installing Semi-Structued PDF Loader on my machine, so i used `UnstructuredPDFLoader` instead).<br>\n",
        "There are also wide of Retrieval Strategies (<a href=\"https://medium.com/@abhinavkimothi/rag-value-chain-retrieval-strategies-in-information-augmentation-for-large-language-models-3a44845e1e26\">as shown on this medium blog</a>) than only Multi-Query Retrieval alone, it is good to explore all of Retrieval technique for comparing and testing RAG results. Finally the RAGAS evaluation is also needs to be improved on open-source LLMs (as i'm facing issue generating the eval scores)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
